<img src="https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png" width="50" height="50">

# Project 2: Statistical Modeling and Model Validation

---

## Due Date
Part 1 will be due **Friday, February 12** before midnight.

The remainder of this project will be due **Friday, February 19**.

Submissions must be made via a push to _this_ repository on your _own_ fork.

## Materials We Provide

| Topic | Description | Link |
| --- | --- | --- |
| Starter Code | Project Description | [Here](modeling-project.ipynb) |
| Car MPG data | Highway MPGs of many popular cars | The `data` folder |
| Diamond data | Measurements for thousands of diamonds | The `data` folder |

---

## Project Objective

In this project, you'll build a linear regression (and KNN) model using guided prompts. You will be exposed to model validation using the train/test split and cross-validation in order to fit different models.

For this project, you will be:
  - Creating a train/test split of the data.
  - Training each of your models on the training data.
  - Evaluating each of the models on the test data.
  - Ranking models by how well they scored on the testing data set.

---

## Project Requirements

In a Jupyter Notebook, create working solutions for all of the **required** questions. Your notebook should include:

1. Text for each question, copy and pasted from the starter code provided.
2. A working solution to each problem.
   - Do not include test, practice, or broken code (*unless you were unable to create a working solution*).
3. Comments for all of your code.
   - In your comments, describe any assumptions you made in order to solve these problems.

4. **Bonus**: After completing the required portions, try your hand at the **bonus** sections for some additional challenges!

---

## Rubric

For all projects, requirements will be evaluated on a simple point scale of 0, 1, or 2. Additionally, instructors will provide you with feedback on required portions of your project.

To determine your final score, we use the following metrics to determine the quality of your project:
- Explanation - Is your code explained properly? If the code doesn't need explanation, is it interpretted properly?
- Correctness - Did you get the right answer?
- Code Quality - Are you writing code that follows the proper paradigms learned in class?
- Code Readability - Are you following code best practices? Is your code readable to another data scientist? This may include comments for explaining difficult-to-read code.

| Score | Expectations |
| ---   | ---          |
| 0	| Incomplete. |
| 1 | You gave a solid effort, but the code does not run and/or yield correct results. |
| 2	| Code runs and the correct answer is given. Good job! |
| 3 | Code and bonus were both successfully completed. Or, code achieves its task in a way that is easier, faster, or more efficient than what you might have done given the material taught in class. Wow! |

_Note: Scores of 2 mean that a requirement has been completely fulfilled, while 3 is typically reserved for bonus objectives._

---

## Submission

Your instructor will explain how to submit your assignment. Typically, this is done either by:

- Creating a repository in your github profile, hosting your materials, and sharing a link with your instructor. [or]
- Forking the project repository, adding your solutions, and submitting a pull request back to the relevant repo.
- Make sure to provide interpretation for all outputs.

---
