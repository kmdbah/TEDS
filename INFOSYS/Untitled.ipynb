{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\614318\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\614318\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Text in the data? Yes/No:  Yes\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-29e6246d313e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;31m#Get TFIDF Scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0msklearn_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0msklearn_representation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mText_Column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[0mTfidf_Output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn_representation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msklearn_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[0m\u001b[0;32m   1218\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1088\u001b[1;33m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[0;32m   1089\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "# The training data file name has to be provided in line 172, validation data in 176\n",
    "# Dependent variable should be named \"binary\".Text data, when present, should be the last column, and named \"text\"\n",
    "# The confusion matrix will be in the output file nn_validation_confusion.png and validation data probabilities in Validation_NN.csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "import pandas\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "#Assumptions - Dependent variable is named Binary, Text is the last column and is named text\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "i=0\n",
    "\n",
    "#Plotting Confusion Matrix\n",
    "def show_confusion_matrix(C,class_labels=['0','1']):\n",
    "\n",
    "    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n",
    "\n",
    "    # true negative, false positive, etc...\n",
    "    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n",
    "\n",
    "    NP = fn+tp # Num positive examples\n",
    "    NN = tn+fp # Num negative examples\n",
    "    N  = NP+NN\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax  = fig.add_subplot(111)\n",
    "    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n",
    "\n",
    "    # Draw the grid boxes\n",
    "    ax.set_xlim(-0.5,2.5)\n",
    "    ax.set_ylim(2.5,-0.5)\n",
    "    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n",
    "    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n",
    "    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n",
    "    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n",
    "\n",
    "    # Set xlabels\n",
    "    ax.set_xlabel('Predicted Label', fontsize=16)\n",
    "    ax.set_xticks([0,1,2])\n",
    "    ax.set_xticklabels(class_labels + [''])\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.xaxis.tick_top()\n",
    "    # These coordinate might require some tinkering. Ditto for y, below.\n",
    "    ax.xaxis.set_label_coords(0.34,1.06)\n",
    "\n",
    "    # Set ylabels\n",
    "    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n",
    "    ax.set_yticklabels(class_labels + [''],rotation=90)\n",
    "    ax.set_yticks([0,1,2])\n",
    "    ax.yaxis.set_label_coords(-0.09,0.65)\n",
    "\n",
    "\n",
    "    # Fill in initial metrics: tp, tn, etc...\n",
    "    ax.text(0,0,\n",
    "            '%d'%(tn),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(0,1,\n",
    "            '%d'%fn,\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1,0,\n",
    "            '%d'%fp,\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "\n",
    "    ax.text(1,1,\n",
    "            '%d'%(tp),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    # Fill in secondary metrics: accuracy, true pos rate, etc...\n",
    "    ax.text(2,0,\n",
    "            'Error: %.2f'%(fp / (fp+tn+0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(2,1,\n",
    "            'Error: %.2f'%(fn / (tp+fn+0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(2,2,\n",
    "            'Accuracy: %.2f'%((tp+tn+0.)/N),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(0,2,' ',\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1,2,\n",
    "            ' ',\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    no_punct = \"\"\n",
    "    for letter in s:\n",
    "        if letter not in string_punctuation:\n",
    "            no_punct += letter\n",
    "    return no_punct\n",
    "\n",
    "#Calculate Lift\n",
    "def calc_Decile(y_pred,y_actual,y_prob,bins=10):\n",
    "    cols = ['ACTUAL','PROB_POSITIVE','PREDICTED']\n",
    "    data = [y_actual,y_prob[:,1],y_pred]\n",
    "    dfa = pandas.DataFrame(dict(zip(cols,data)))\n",
    "\n",
    "    #Observations where y=1\n",
    "    total_positive_n = dfa['ACTUAL'].sum()\n",
    "    #Total Observations\n",
    "    dfa= dfa.reset_index()\n",
    "    total_n = dfa.index.size\n",
    "    natural_positive_prob = total_positive_n/float(total_n)\n",
    "    dfa = dfa.sort_values(by=['PROB_POSITIVE'], ascending=[False])\n",
    "    dfa['rank'] = dfa['PROB_POSITIVE'].rank(method='first')\n",
    "    #Create Bins where First Bin has Observations with the\n",
    "    #Highest Predicted Probability that y = 1\n",
    "    dfa['BIN_POSITIVE'] = pandas.qcut(dfa['rank'],bins,labels=False)\n",
    "    pos_group_dfa = dfa.groupby('BIN_POSITIVE')\n",
    "    #Percentage of Observations in each Bin where y = 1\n",
    "    lift_positive = pos_group_dfa['ACTUAL'].sum()/pos_group_dfa['ACTUAL'].count()\n",
    "    lift_index_positive = lift_positive/natural_positive_prob\n",
    "\n",
    "    #result1 = result.reset_index()\n",
    "    #Consolidate Results into Output Dataframe\n",
    "    lift_df = pandas.DataFrame({'LIFT_POSITIVE':lift_positive,\n",
    "                               'LIFT_POSITIVE_INDEX':lift_index_positive,\n",
    "                               'BASELINE_POSITIVE':natural_positive_prob})\n",
    "\n",
    "    return lift_df\n",
    "\n",
    "\n",
    "\n",
    "#Read file\n",
    "##Training Data\n",
    "df = pandas.read_csv('High Note data.csv',encoding=\"ISO-8859-1\")\n",
    "\n",
    "##Validation Data\n",
    "df_test = pandas.read_csv('High Note data.csv',encoding=\"ISO-8859-1\")\n",
    "\n",
    "Text_present = input('Text in the data? Yes/No: ')\n",
    "if Text_present =='Yes':\n",
    "    #Read the text column---Last Column (Assumption)\n",
    "    string_punctuation = '''()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890'''\n",
    "    stop = stopwords.words('english')\n",
    "    df.iloc[ :, -1] = df.iloc[ :, -1].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    for row in df['text']:\n",
    "        df.iloc[ i, -1] = remove_punctuation(row)\n",
    "        i=i+1\n",
    "    df['text'] = df['text'].str.replace(\"!\",\" !\")\n",
    "    df['text'] = df['text'].apply(word_tokenize)\n",
    "    df['text'] = df['text'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    df['text'] = df['text'].apply(lambda x : \" \".join(x))\n",
    "    Text_Column = df.iloc[ :, -1:]\n",
    "    #Get TFIDF Scores\n",
    "    sklearn_tfidf = TfidfVectorizer(min_df=.01, max_df =.95, stop_words=\"english\",use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "    sklearn_representation = sklearn_tfidf.fit_transform(Text_Column.iloc[:, 0].tolist())\n",
    "    Tfidf_Output = pandas.DataFrame(sklearn_representation.toarray(), columns=sklearn_tfidf.get_feature_names())\n",
    "\n",
    "    #Append the column to the final dataset\n",
    "    Input = pandas.concat([df, Tfidf_Output], axis=1)\n",
    "    Input = Input.drop('text', 1)\n",
    "else:\n",
    "    Input = df\n",
    "\n",
    "if Text_present =='Yes':\n",
    "    #Read the text column---Last Column (Assumption)\n",
    "    string_punctuation = '''()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890'''\n",
    "    stop = stopwords.words('english')\n",
    "    df_test.iloc[ :, -1] = df_test.iloc[ :, -1].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    for row in df_test['text']:\n",
    "        df_test.iloc[ i, -1] = remove_punctuation(row)\n",
    "        i=i+1\n",
    "    df_test['text'] = df_test['text'].str.replace(\"!\",\" !\")\n",
    "    df_test['text'] = df_test['text'].apply(word_tokenize)\n",
    "    df_test['text'] = df_test['text'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    df_test['text'] = df_test['text'].apply(lambda x : \" \".join(x))\n",
    "    Text_Column = df_test.iloc[ :, -1:]\n",
    "    #Get TFIDF Scores\n",
    "    sklearn_tfidf = TfidfVectorizer(min_df=.01, max_df =.95, stop_words=\"english\",use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "    sklearn_representation = sklearn_tfidf.fit_transform(Text_Column.iloc[:, 0].tolist())\n",
    "    Tfidf_Output = pandas.DataFrame(sklearn_representation.toarray(), columns=sklearn_tfidf.get_feature_names())\n",
    "\n",
    "    #Append the column to the final dataset\n",
    "    Input_test = pandas.concat([df_test, Tfidf_Output], axis=1)\n",
    "    Input_test = Input.drop('text', 1)\n",
    "else:\n",
    "    Input_test = df_test\n",
    "\n",
    "X = Input.loc[:, Input.columns != 'binary']\n",
    "Y = Input['binary']\n",
    "X_Validation_Score = X\n",
    "\n",
    "\n",
    "X_test = Input_test.loc[:, Input_test.columns != 'binary']\n",
    "Y_test = Input_test['binary']\n",
    "X_test_Validation_Score = X_test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, train_size=0.6, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transformations to the data:\n",
    "# X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X = scaler.transform(X)\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(25,25,25)).fit(X,Y)\n",
    "#mlp.fit(X,Y)\n",
    "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(25, 25, 25), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=30, momentum=0.6,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.01, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "\n",
    "#Prediction\n",
    "#predictions = mlp.predict(X_test)\n",
    "\n",
    "Y_pred = model_selection.cross_val_predict(classifier, X_test, Y_test, cv=5)\n",
    "confusion_matrix=confusion_matrix(np.array(Y_test),Y_pred)\n",
    "\n",
    "#Validation score table\n",
    "y_prob = cross_val_predict(classifier, X_test, Y_test, method='predict_proba')\n",
    "\n",
    "\n",
    "validation_columns = ['Predicted_Probability','Y','Y_pred']\n",
    "validation_data = [y_prob[:,1],Y_test,Y_pred]\n",
    "Validation_NN = pandas.DataFrame(dict(zip(validation_columns,validation_data)))\n",
    "Validation_NN = pandas.concat([Validation_NN, X_test_Validation_Score], axis=1)\n",
    "Validation_NN = Validation_NN.sort_values(by=['Predicted_Probability'], ascending=[False])\n",
    "Validation_NN.loc[Validation_NN['Predicted_Probability']>0.4999999, 'Y_pred'] = \"1\"\n",
    "Validation_NN.loc[Validation_NN['Predicted_Probability']<0.5, 'Y_pred'] = \"0\"\n",
    "\n",
    "\n",
    "\n",
    "#Decile chart\n",
    "Decile_Chart = calc_Decile(Y_pred,Y_test,y_prob)\n",
    "Decile_Chart['Bin']=abs(10-Decile_Chart.index)\n",
    "Decile_Chart = Decile_Chart.sort_values(by=['Bin'], ascending=[False])\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.bar(Decile_Chart['Bin'], Decile_Chart['LIFT_POSITIVE_INDEX'], align='center',)\n",
    "plt.xlabel('Bins')\n",
    "plt.title('Decile Chart')\n",
    "plt.xticks(Decile_Chart['Bin'])\n",
    "\n",
    "# Lift chart\n",
    "cols = ['ACTUAL','PROB_POSITIVE','PREDICTED']\n",
    "data = [Y_test,y_prob[:,1],Y_pred]\n",
    "Lift_data = pandas.DataFrame(dict(zip(cols,data)))\n",
    "Lift_data = Lift_data.sort_values(by=['PROB_POSITIVE'], ascending=[False])\n",
    "Lift_data['cum_actual'] = Lift_data.ACTUAL.cumsum()\n",
    "\n",
    "Lift_data = Lift_data.reset_index()\n",
    "del Lift_data ['index']\n",
    "p = Lift_data['cum_actual']\n",
    "d = Lift_data.index+1\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(d,p,color='blue',marker='o',markersize=.2)\n",
    "total_positive_n = Lift_data['ACTUAL'].sum()\n",
    "total_positive_count = Lift_data['ACTUAL'].count()\n",
    "plt.plot([1,total_positive_count],[1,total_positive_n],color='red',marker='o')\n",
    "\n",
    "plt.legend(['Cumulative 1 when sorted using predicted values'])\n",
    "plt.title(\"Lift Chart\")\n",
    "plt.xlabel(\"#Cases\")\n",
    "plt.grid()\n",
    "plt.savefig('NN_Decile_Lift.png')\n",
    "show_confusion_matrix(confusion_matrix, ['0', '1'])\n",
    "plt.show()\n",
    "plt.savefig('NN_Validation_Confusion.png')\n",
    "\n",
    "Validation_NN = Validation_NN.rename(columns={'Predicted_Probability': 'Prob of 1', 'Y_pred': 'Predicted', 'Y': 'Actual'})\n",
    "Validation_NN.to_csv('Validation_NN.csv', index_label=[Validation_NN.columns.name], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
